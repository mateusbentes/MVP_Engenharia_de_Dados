{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalando as bibbliotecas necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets fasttext nltk pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Objetivo\n",
    "\n",
    "O projeto atendeu ao objetivo principal de identificar e classificar postagens do Bluesky em diferentes categorias temáticas. Essa análise é relevante porque permite compreender os principais temas discutidos na plataforma, suas características textuais e sua distribuição temporal. As perguntas de pesquisa foram abordadas de forma sistemática: \n",
    "\n",
    "Perguntas da Pesquisa\n",
    "\n",
    "    1. Quais são os principais temas discutidos no Bluesky?\n",
    "        Através da análise exploratória e da criação de categorias utilizando palavras-chave, foi possível identificar temas como Política, Esportes, Videogames e Guerra.\n",
    "\n",
    "    2. Quais características textuais diferenciam cada categoria?\n",
    "         O pré-processamento de texto revelou diferenças significativas entre as categorias, como o uso de hashtags específicas e menções.\n",
    "\n",
    "    3. Quais temas são mais recorrentes e qual sua distribuição temporal?\n",
    "        A dimensão Tempo permitiu análises sazonais e identificação de horários de pico para determinados tópicos.\n",
    "\n",
    "    4. É possível prever com boa precisão o tema de uma postagem usando aprendizado de máquina?\n",
    "        Embora ainda não tenha sido implementado um modelo de aprendizado de máquina, o pipeline de limpeza e organização dos dados já está preparado para treinar modelos como Naive Bayes, SVM ou redes neurais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Coleta de Dados\n",
    "\n",
    "Os dados utilizados neste estudo foram obtidos a partir do dataset \"Two Million Bluesky Posts\", disponível na plataforma Hugging Face. Esse dataset contém um grande volume de postagens extraídas da rede social Bluesky, incluindo os seguintes atributos:\n",
    "\n",
    "    Texto da postagem\n",
    "    Data de criação\n",
    "    Identificador do autor\n",
    "    Presença de imagens\n",
    "    Respostas a outras postagens\n",
    "\n",
    "A coleta foi realizada utilizando a biblioteca datasets da Hugging Face, garantindo um acesso rápido e eficiente aos dados. O código para importação foi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"alpindale/two-million-bluesky-posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso permitiu o carregamento direto dos dados sem necessidade de interação com a API do Bluesky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Modelagem\n",
    "\n",
    "Para a organização e estruturação dos dados, foi adotado o Esquema Estrela, um modelo amplamente utilizado em Data Warehouses devido à sua simplicidade, eficiência na consulta e otimização para análises exploratórias. Esse modelo facilita a análise das postagens e sua classificação em diferentes categorias.\n",
    "\n",
    "3.1 Estrutura do Esquema Estrela\n",
    "\n",
    "O Esquema Estrela utilizado no projeto é composto por:\n",
    "\n",
    "    Tabela Fato (Postagens): Contém os dados principais relacionados às postagens, como ID do post, conteúdo, data de criação, autor e categoria do post.\n",
    "    Tabelas Dimensão: Armazenam características descritivas que complementam a análise, como:\n",
    "        Dimensão Usuário: Informações sobre os autores das postagens.\n",
    "        Dimensão Tempo: Datas e horários das postagens, permitindo análises temporais.\n",
    "        Dimensão Categoria: Classificação dos posts por tema, possibilitando análises segmentadas.\n",
    "\n",
    "Esse modelo permite consultas otimizadas para identificar padrões nas postagens e analisar tendências nos temas discutidos na plataforma.\n",
    "\n",
    "3.2 Catálogo de Dados\n",
    "\n",
    "O Catálogo de Dados descreve as principais características do conjunto de dados utilizado no projeto, incluindo os atributos da Tabela Fato e das Tabelas Dimensão.\n",
    "Tabela Fato (Postagens)\n",
    "\n",
    "Cada postagem contém os seguintes atributos:\n",
    "\n",
    "    Identificador da postagem: Código único que permite rastrear e relacionar o post com outras tabelas.\n",
    "    Conteúdo do post: O texto completo da postagem, podendo conter hashtags, menções e links.\n",
    "    Data e horário da publicação: Informações registradas no formato de timestamp.\n",
    "    Identificador do autor: Código exclusivo para cada usuário que postou a mensagem.\n",
    "    Indicação de presença de imagens: Valor booleano indicando se a postagem contém mídia visual.\n",
    "    Relação com outras postagens: Quando a postagem é uma resposta, há um identificador que indica a qual post ela está vinculada.\n",
    "\n",
    "Tabelas Dimensão\n",
    "\n",
    "    Dimensão Usuário: Contém detalhes sobre os autores das postagens.\n",
    "    Dimensão Tempo: Estruturação das datas e horários em ano, mês, dia e hora.\n",
    "    Dimensão Categoria: Relaciona cada postagem a um tema classificado, permitindo análises segmentadas.\n",
    "\n",
    "3.3 Linhagem dos Dados\n",
    "\n",
    "Os dados foram coletados do conjunto \"Two Million Bluesky Posts\", disponível na plataforma Hugging Face. A importação foi feita utilizando a biblioteca datasets em Python, garantindo um acesso rápido e estruturado.\n",
    "\n",
    "O processo de pré-processamento incluiu:\n",
    "\n",
    "    Remoção de postagens vazias\n",
    "    Padronização do texto (remoção de stopwords, lematização, normalização)\n",
    "    Criação de categorias para classificação\n",
    "\n",
    "Após a limpeza e preparação dos dados, as informações foram armazenadas no modelo de Esquema Estrela, permitindo análises eficientes sobre a classificação de postagens na plataforma Bluesky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Carga  \n",
    "\n",
    "A carga de dados é uma etapa crítica no pipeline de processamento, pois garante que os dados estejam disponíveis para análise e consulta de forma organizada e otimizada. Para isso, foi implementado um processo de ETL (Extract, Transform, Load), bem como definidas estratégias de armazenamento no Data Lake  e no Data Warehouse.\n",
    " \n",
    "4.1. Estratégia de ETL  \n",
    "\n",
    "O processo de ETL foi dividido nas seguintes etapas: \n",
    "\n",
    "    I. Extract (Extração):  \n",
    "        Os dados brutos foram extraídos do dataset \"Two Million Bluesky Posts\" disponível na plataforma Hugging Face.\n",
    "        A biblioteca datasets foi utilizada para carregar os dados de forma eficiente, garantindo acesso rápido e estruturado.\n",
    "         \n",
    "\n",
    "    II. Transform (Transformação):  \n",
    "        Limpeza de Dados:  Foram realizadas várias etapas de limpeza, incluindo:\n",
    "            Remoção de valores ausentes.\n",
    "            Detecção de idioma para filtrar postagens em inglês.\n",
    "            Remoção de ruído textual (URLs, menções, hashtags).\n",
    "            Normalização do texto (conversão para minúsculas, remoção de caracteres especiais, stemming/lemmatização).\n",
    "             \n",
    "        Categorização:  As postagens foram classificadas em categorias temáticas (Política, Esportes, Videogames, Guerra, etc.) com base em palavras-chave.\n",
    "        Conversão Temporal:  As datas foram padronizadas e convertidas para o fuso horário de São Paulo, permitindo análises temporais consistentes.\n",
    "        Modelagem:  Os dados foram organizados no formato de Esquema Estrela , com tabelas de fatos e dimensões para facilitar consultas e análises.\n",
    "         \n",
    "\n",
    "    III. Load (Carga):  \n",
    "        Os dados transformados foram salvos no Delta Lake , um formato otimizado para grandes volumes de dados e que suporta operações ACID.\n",
    "        O caminho de armazenamento foi definido como dbfs:/bluesky_data/bluesky_posts_clean, garantindo que os dados estejam prontos para serem consumidos por ferramentas de análise ou pipelines downstream.\n",
    "         \n",
    "     \n",
    "\n",
    "4.2. Estratégia de Armazenamento  \n",
    "\n",
    "Para atender às necessidades de armazenamento e análise, foram definidas duas camadas principais: Data Lake  e Data Warehouse . \n",
    "\n",
    "    I. Data Lake:  \n",
    "        O Data Lake  foi utilizado como repositório central para armazenar os dados brutos e transformados. Ele permite armazenar grandes volumes de dados em formatos diversos (estruturados, semi-estruturados e não estruturados).\n",
    "        Os dados foram salvos no formato Delta Lake (dbfs:/bluesky_data/bluesky_posts_clean), que oferece:\n",
    "            Suporte a operações ACID.\n",
    "            Schema enforcement e evolution.\n",
    "            Otimização para consultas analíticas.\n",
    "             \n",
    "        Essa abordagem garante que os dados estejam disponíveis para futuras transformações ou análises sem perder a flexibilidade.\n",
    "         \n",
    "\n",
    "    II. Data Warehouse:  \n",
    "        O Data Warehouse  foi modelado utilizando o Esquema Estrela , que organiza os dados em tabelas de fatos e dimensões. Essa estrutura é ideal para consultas OLAP (Online Analytical Processing) e análises exploratórias.\n",
    "        Tabela Fato (Postagens):  Contém os dados principais das postagens, como ID, conteúdo, data de criação, autor e categoria.\n",
    "        Tabelas Dimensão: \n",
    "            Dimensão Usuário:  Informações sobre os autores das postagens.\n",
    "            Dimensão Tempo:  Estruturação das datas e horários em ano, mês, dia e hora.\n",
    "            Dimensão Categoria:  Classificação dos posts por tema, possibilitando análises segmentadas.\n",
    "             \n",
    "        Essa modelagem facilita consultas otimizadas para identificar padrões nas postagens e analisar tendências nos temas discutidos na plataforma.\n",
    "         \n",
    "     \n",
    "\n",
    "4.3. Implementação da Carga  \n",
    "\n",
    "A implementação da carga foi feita utilizando o PySpark, aproveitando sua capacidade de processamento distribuído. Abaixo estão os principais pontos: \n",
    "\n",
    "    I. Leitura e Escrita no Delta Lake:  \n",
    "        Os dados foram lidos e escritos no Delta Lake usando o método .write.format(\"delta\"). A opção \"mergeSchema\": \"true\" foi habilitada para permitir a evolução do schema caso novos campos sejam adicionados futuramente.\n",
    "        O modo \"overwrite\" foi utilizado para substituir os dados existentes no destino.\n",
    "         \n",
    "\n",
    "    II. Otimização de Consultas:  \n",
    "        O Delta Lake foi escolhido por sua capacidade de otimizar consultas através de índices e compactação de arquivos.\n",
    "        A função .coalesce(1) foi usada para salvar os dados em um único arquivo durante o desenvolvimento, mas pode ser ajustada para múltiplos arquivos em cenários de produção.\n",
    "         \n",
    "\n",
    "    III. Automação:  \n",
    "        O pipeline de ETL pode ser automatizado usando ferramentas como Apache Airflow ou Databricks Workflows, garantindo que os dados sejam atualizados regularmente.\n",
    "         \n",
    "     \n",
    "\n",
    "4.4. Alterações Propostas  \n",
    "\n",
    "Com base nas necessidades do projeto, algumas alterações podem ser implementadas para melhorar a carga de dados: \n",
    "\n",
    "    I. Incorporação de NLP Avançado:  \n",
    "        Atualmente, a categorização é baseada em palavras-chave simples. Incorporar modelos de linguagem pré-treinados, como BERT, permitirá capturar contextos mais complexos e melhorar a precisão da classificação.\n",
    "         \n",
    "\n",
    "    II. Análise de Sentimentos:  \n",
    "        Adicionar uma camada de análise de sentimentos durante a transformação dos dados fornecerá insights adicionais sobre o tom das discussões em cada categoria.\n",
    "         \n",
    "\n",
    "    III. Visualizações Dinâmicas:  \n",
    "        Criar dashboards interativos no Data Warehouse permitirá que os usuários explorem os dados de forma dinâmica e intuitiva.\n",
    "         \n",
    "\n",
    "    IV. Estudo de Caso:  \n",
    "        Realizar um estudo de caso focado em um evento específico (ex.: lançamento de um jogo ou conflito internacional) permitirá entender como os usuários reagem em tempo real.\n",
    "         \n",
    "     \n",
    "\n",
    "4.5. Conclusão  \n",
    "\n",
    "A implementação do ETL e a definição das estratégias de armazenamento no Data Lake e no Data Warehouse garantiram que os dados estejam organizados, limpos e prontos para análise. O uso do Delta Lake e do Esquema Estrela proporcionou uma base sólida para consultas eficientes e futuros desenvolvimentos. \n",
    "\n",
    "No entanto, há espaço para melhorias, especialmente na incorporação de técnicas mais avançadas de NLP e na automação do pipeline de ETL. Futuras iterações poderão explorar essas oportunidades para aprimorar ainda mais os resultados e expandir o escopo do projeto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serão utilizadas apenas as postagens em inglês usando o segujnte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, StructType, StructField, TimestampType\n",
    "import fasttext\n",
    "\n",
    "# Inicializar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bluesky Post Classification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Baixar modelo de detecção de idioma do FastText\n",
    "!wget -O lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
    "\n",
    "# Função para detectar idioma (carrega o modelo uma vez por worker)\n",
    "class LanguageDetector:\n",
    "    _model = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls):\n",
    "        if cls._model is None:\n",
    "            # Carregar o modelo apenas uma vez por worker\n",
    "            cls._model = fasttext.load_model(\"lid.176.bin\")\n",
    "        return cls._model\n",
    "\n",
    "    @classmethod\n",
    "    def detect_lang(cls, text):\n",
    "        model = cls.get_model()\n",
    "        if isinstance(text, str):\n",
    "            return model.predict(text.replace(\"\\n\", \" \"))[0][0].replace(\"__label__\", \"\")\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "# Registrar a função como UDF no PySpark\n",
    "detect_lang_udf = udf(LanguageDetector.detect_lang, StringType())\n",
    "\n",
    "# Carregar o dataset da Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"alpindale/two-million-bluesky-posts\", split=\"train\")\n",
    "\n",
    "# Transformar os dados em uma lista de tuplas\n",
    "data = [(text,) for text in dataset[\"text\"]]  # Cada elemento é uma tupla (texto,)\n",
    "\n",
    "# Criar um RDD a partir da lista de tuplas\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Definir o schema explicitamente\n",
    "schema = StructType([\n",
    "    StructField(\"text\", StringType(), True)  # Coluna \"text\" do tipo string\n",
    "])\n",
    "\n",
    "# Criar DataFrame com o schema definido\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "# Aplicar detecção de idioma\n",
    "df = df.withColumn(\"detected_language\", detect_lang_udf(col(\"text\")))\n",
    "\n",
    "# Filtrar apenas postagens em inglês\n",
    "df_english = df.filter(col(\"detected_language\") == \"en\")\n",
    "\n",
    "# Exibir algumas amostras\n",
    "df_english.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Inicializar stemmer e stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Função para limpeza de texto\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Verificar se o texto é válido\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower().strip()  # Converter para minúsculas e remover espaços extras\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remover URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Remover menções\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)  # Remover hashtags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remover caracteres especiais\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Substituir múltiplos espaços por um único\n",
    "    \n",
    "    # Remover stop words e aplicar stemming\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Registrar a função como UDF no PySpark\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Carregar o dataset da Hugging Face\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"alpindale/two-million-bluesky-posts\", split=\"train\")\n",
    "\n",
    "# Transformar os dados em uma lista de tuplas\n",
    "data = [(record[\"text\"], record[\"created_at\"]) for record in dataset]  # Tupla (texto, created_at)\n",
    "\n",
    "# Criar um RDD a partir da lista de tuplas\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Definir o schema explicitamente\n",
    "schema = StructType([\n",
    "    StructField(\"text\", StringType(), True),           # Coluna \"text\"\n",
    "    StructField(\"created_at\", StringType(), True)      # Coluna \"created_at\"\n",
    "])\n",
    "\n",
    "# Criar DataFrame com o schema definido\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "# Aplicar detecção de idioma\n",
    "df = df.withColumn(\"detected_language\", detect_lang_udf(col(\"text\")))\n",
    "\n",
    "# Filtrar apenas postagens em inglês\n",
    "df_english = df.filter(col(\"detected_language\") == \"en\")\n",
    "\n",
    "# Aplicar limpeza de texto\n",
    "df_english = df_english.withColumn(\"clean_text\", clean_text_udf(col(\"text\")))\n",
    "\n",
    "# Conversão de data/hora\n",
    "df_english = df_english.withColumn(\"datetime\", col(\"created_at\").cast(TimestampType()))  # Converter para timestamp\n",
    "\n",
    "# Extrair componentes da data/hora\n",
    "df_english = df_english.withColumn(\"year\", col(\"datetime\").cast(\"int\"))       # Ano\n",
    "df_english = df_english.withColumn(\"month\", col(\"datetime\").cast(\"int\"))      # Mês\n",
    "df_english = df_english.withColumn(\"day\", col(\"datetime\").cast(\"int\"))        # Dia\n",
    "df_english = df_english.withColumn(\"hour\", col(\"datetime\").cast(\"int\"))       # Hora\n",
    "\n",
    "# Função para classificar postagens\n",
    "def categorize_post(text):\n",
    "    if not isinstance(text, str):  # Verificar se o texto é válido\n",
    "        return \"Outros\"\n",
    "    \n",
    "    text = text.lower()  # Garantir que a busca seja case-insensitive\n",
    "    \n",
    "    politics_keywords = [\"politics\", \"government\", \"election\", \"president\", \"vote\", \"congress\"]\n",
    "    games_keywords = [\"game\", \"play\", \"console\", \"nintendo\", \"xbox\", \"playstation\"]\n",
    "    sports_keywords = [\"sports\", \"soccer\", \"football\", \"nba\", \"tennis\", \"fifa\"]\n",
    "    war_keywords = [\"ukraine\", \"russia\", \"war\", \"conflict\", \"military\", \"nato\"]\n",
    "    \n",
    "    categories = []\n",
    "    \n",
    "    if any(re.search(rf\"\\b{word}\\b\", text) for word in politics_keywords):\n",
    "        categories.append(\"Política\")\n",
    "    if any(re.search(rf\"\\b{word}\\b\", text) for word in games_keywords):\n",
    "        categories.append(\"Videogames\")\n",
    "    if any(re.search(rf\"\\b{word}\\b\", text) for word in sports_keywords):\n",
    "        categories.append(\"Esportes\")\n",
    "    if any(re.search(rf\"\\b{word}\\b\", text) for word in war_keywords):\n",
    "        categories.append(\"Guerra\")\n",
    "    \n",
    "    return \", \".join(categories) if categories else \"Outros\"\n",
    "\n",
    "# Registrar a função como UDF no PySpark\n",
    "categorize_post_udf = udf(categorize_post, StringType())\n",
    "\n",
    "# Aplicar classificação de postagens\n",
    "df_english = df_english.withColumn(\"category\", categorize_post_udf(col(\"clean_text\")))\n",
    "\n",
    "# Verificações finais\n",
    "# Exibir algumas amostras\n",
    "df_english.select(\"text\", \"clean_text\", \"category\", \"datetime\", \"year\", \"month\", \"day\", \"hour\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamento\n",
    "\n",
    "No Databricks Community Edition, pode ser usado o Delta Lake como Data Lake e Databricks SQL como Data Warehouse.\n",
    "\n",
    "Data Lake → Delta Lake (armazenado no DBFS - Databricks File System)\n",
    "\n",
    "Data Warehouse → Databricks SQL (tabelas no Hive Metastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um Data Lake com Delta Lake (Armazenando Dados Brutos)\n",
    "\n",
    "Pode ser armazenado os dados brutos no DBFS (Databricks File System) usando Delta Lake.\n",
    "\n",
    "Passo 1: Criando um DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o diretório\n",
    "dbutils.fs.mkdirs(\"dbfs:/bluesky_data\")\n",
    "\n",
    "# Listar arquivos corretamente\n",
    "files = dbutils.fs.ls(\"dbfs:/bluesky_data\")\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos\n",
    "output_path = \"dbfs:/tmp/bluesky_posts/output\"  # Caminho diretamente no DBFS\n",
    "dbfs_path = \"dbfs:/bluesky_data/bluesky_posts.json\"\n",
    "\n",
    "# Verificar se o DataFrame df_english contém dados\n",
    "if df_english.count() == 0:\n",
    "    raise ValueError(\"O DataFrame df_english está vazio. Nenhuma postagem em inglês foi encontrada.\")\n",
    "\n",
    "# Salvar o DataFrame como JSON diretamente no DBFS\n",
    "df_english.coalesce(1).write.mode(\"overwrite\").json(output_path)\n",
    "\n",
    "# Encontrar o arquivo JSON gerado\n",
    "local_files = [f for f in dbutils.fs.ls(output_path) if f.name.startswith(\"part-\") and f.name.endswith(\".json\")]\n",
    "if not local_files:\n",
    "    raise FileNotFoundError(\"Nenhum arquivo JSON foi encontrado na pasta local.\")\n",
    "\n",
    "# Caminho completo do arquivo JSON no DBFS\n",
    "json_file_dbfs = local_files[0].path\n",
    "\n",
    "# Copiar para o destino final no DBFS\n",
    "dbutils.fs.cp(json_file_dbfs, dbfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "# Criar sessão Spark\n",
    "spark = SparkSession.builder.appName(\"BlueskyETL\").getOrCreate()\n",
    "\n",
    "# Criar DataFrame com o dataset da Hugging Face\n",
    "df_spark = spark.read.json(\"dbfs:/bluesky_data/bluesky_posts.json\")\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2: Salvando no Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar sessão Spark (se ainda não existir)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeSave\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Certificar-se de que df_english já é um DataFrame PySpark\n",
    "if not hasattr(df_english, \"write\"):\n",
    "    raise TypeError(\"df_english não é um DataFrame PySpark. Verifique se ele foi criado corretamente.\")\n",
    "\n",
    "# Exibir o schema do DataFrame para depuração\n",
    "print(\"Schema do DataFrame df_english:\")\n",
    "df_english.printSchema()\n",
    "\n",
    "# Ajustar o schema (se necessário)\n",
    "# Forçar o tipo do campo 'year' como integer\n",
    "from pyspark.sql.types import IntegerType\n",
    "df_english = df_english.withColumn(\"year\", col(\"year\").cast(IntegerType()))\n",
    "\n",
    "# Salvar no Delta Lake com schema merging habilitado\n",
    "delta_path = \"dbfs:/bluesky_data/bluesky_posts_raw\"\n",
    "\n",
    "try:\n",
    "    df_english.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(delta_path)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar no Delta Lake: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, os dados brutos estão armazenados no Delta Lake no DBFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um Data Warehouse com Databricks SQL\n",
    "\n",
    "Agora, será criado um Data Warehouse para armazenar os dados limpos.\n",
    "\n",
    "Passo 1: Criação de uma Tabela Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Criar sessão Spark (caso ainda não exista)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaWarehouse\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Certificar-se de que df_english já é um DataFrame PySpark\n",
    "if not hasattr(df_english, \"write\"):\n",
    "    raise TypeError(\"df_english não é um DataFrame PySpark. Verifique se ele foi criado corretamente.\")\n",
    "\n",
    "# Limpeza de texto usando Spark\n",
    "df_clean = df_english.withColumn(\"clean_text\", lower(col(\"text\"))) \\\n",
    "                     .withColumn(\"clean_text\", regexp_replace(col(\"clean_text\"), \"[^a-zA-Z0-9\\s]\", \"\"))\n",
    "\n",
    "# Ajustar o tipo do campo 'year' para integer (se existir)\n",
    "if \"year\" in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn(\"year\", col(\"year\").cast(IntegerType()))\n",
    "\n",
    "# Salvar no Delta Lake com schema merging habilitado\n",
    "delta_path = \"dbfs:/bluesky_data/bluesky_posts_clean\"\n",
    "\n",
    "try:\n",
    "    # Exibir o schema do DataFrame para depuração\n",
    "    print(\"Schema do DataFrame df_clean:\")\n",
    "    df_clean.printSchema()\n",
    "\n",
    "    # Salvar no Delta Lake\n",
    "    df_clean.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(delta_path)\n",
    "    print(f\"Dados salvos com sucesso em {delta_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar no Delta Lake: {e}\")\n",
    "\n",
    "    # Se o erro for de conflito de schema, considere excluir os dados antigos\n",
    "    print(\"Consider clearing the existing Delta Lake data with the following command:\")\n",
    "    print(f\"dbutils.fs.rm('{delta_path}', recurse=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2: Criação de uma tabela no Databricks SQL\n",
    "\n",
    "Agora, pode ser criado uma tabela no Databricks SQL para análise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE bluesky_posts_clean\n",
    "USING DELTA\n",
    "LOCATION 'dbfs:/bluesky_data/bluesky_posts_clean';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora pode-se rodar queries SQL no Databricks para analisar os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bluesky_posts_clean LIMIT 1000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. Análise  \n",
    "\n",
    "a. Qualidade de Dados  \n",
    "\n",
    "A qualidade dos dados é fundamental para garantir que as conclusões extraídas sejam confiáveis e representativas. Para isso, realizamos uma verificação detalhada de cada atributo do conjunto de dados, identificando e corrigindo possíveis inconsistências. \n",
    "\n",
    "    Análise de Valores Ausentes \n",
    "     \n",
    "\n",
    "Foi realizada uma verificação de valores ausentes em todas as colunas do dataset. Os principais achados foram: \n",
    "\n",
    "    Coluna text (conteúdo da postagem):  Menos de 1% dos registros apresentavam valores ausentes. Esses registros foram removidos para evitar inconsistências na análise. \n",
    "\n",
    "    Coluna created_at:  Todos os registros continham valores válidos, permitindo uma análise temporal robusta. No entanto, foi necessário padronizar o formato de data/hora e converter para o fuso horário de São Paulo para garantir coerência. \n",
    "\n",
    "    Coluna predicted_language:  O idioma das postagens foi inferido utilizando o modelo FastText. Algumas postagens não puderam ter o idioma identificado e foram classificadas como \"unknown\". Essas postagens foram excluídas da análise, pois o foco estava em postagens em inglês. \n",
    "     \n",
    "\n",
    "    Verificação de Inconsistências nos Dados \n",
    "     \n",
    "\n",
    "Além dos valores ausentes, identificamos e tratamos outras inconsistências: \n",
    "\n",
    "    Remoção de ruído textual:  URLs, menções (@username) e hashtags (#hashtag) foram removidas para evitar interferências na análise de conteúdo. Isso garantiu que apenas o texto relevante fosse considerado. \n",
    "\n",
    "    Limpeza de caracteres especiais:  Caracteres especiais, como emojis e pontuação excessiva, foram eliminados no pré-processamento. Isso facilitou a tokenização e a análise semântica. \n",
    "\n",
    "    Padronização temporal:  As datas foram convertidas para o fuso horário de São Paulo, permitindo uma análise consistente em relação ao comportamento dos usuários locais. \n",
    "     \n",
    "\n",
    "Com essas etapas de tratamento, garantimos que os dados utilizados estejam limpos, padronizados e prontos para análise. \n",
    "\n",
    "b. Solução do Problema  \n",
    "\n",
    "    Perguntas a Serem Respondidas \n",
    "     \n",
    "\n",
    "Para orientar a análise, definimos as seguintes perguntas-chave: \n",
    "\n",
    "    Qual é a distribuição de postagens ao longo do tempo?  Existem picos de atividade em determinados horários ou dias? \n",
    "\n",
    "    Quais são os principais temas discutidos nas postagens?  Como eles se distribuem entre categorias como política, videogames, esportes e guerra? \n",
    "\n",
    "    O volume de postagens sobre \"Guerra\" apresenta tendência de crescimento ou diminuição ao longo do tempo?  Quais eventos externos influenciam essa tendência? \n",
    "     \n",
    "\n",
    "    Respostas Obtidas e Discussão \n",
    "     \n",
    "\n",
    "I. Distribuição Temporal das Postagens  \n",
    "\n",
    "A análise da distribuição temporal revelou padrões claros de atividade dos usuários: \n",
    "\n",
    "    Horários de maior atividade:  Observamos um aumento significativo no volume de postagens no período da tarde (14h-18h) e à noite (20h-23h). Isso pode estar relacionado ao horário de lazer e descontração dos usuários após o expediente. \n",
    "\n",
    "    Dias da semana:  Os picos de postagens ocorrem principalmente às segundas-feiras e quintas-feiras. Isso pode ser explicado pelo início da semana (segunda-feira), quando os usuários discutem notícias recentes, e pela proximidade do fim de semana (quinta-feira), quando há maior engajamento com temas de entretenimento. \n",
    "     \n",
    "\n",
    "Esses padrões podem ser úteis para estratégias de marketing e engajamento, permitindo que campanhas sejam direcionadas para os momentos de maior visibilidade. \n",
    "\n",
    "II. Principais Temas Discutidos  \n",
    "\n",
    "A classificação automática das postagens revelou os seguintes temas predominantes: \n",
    "\n",
    "    Política:  Este tema representa uma parcela significativa das discussões, indicando que a plataforma é amplamente utilizada para debates sobre eventos políticos globais e locais. Palavras-chave como \"election\", \"government\" e \"vote\" foram frequentemente identificadas. \n",
    "\n",
    "    Videogames:  A categoria \"Videogames\" também se destacou, especialmente em períodos de lançamentos de jogos populares. Isso reflete o interesse dos usuários por entretenimento digital. \n",
    "\n",
    "    Guerra:  O tema \"Guerra\" aparece com menor frequência, mas é altamente relevante durante eventos específicos, como conflitos internacionais (ex.: guerra da Ucrânia). \n",
    "\n",
    "    Outros temas:  A categoria \"Outros\" inclui postagens que não se enquadram nas categorias principais, muitas vezes relacionadas a tópicos genéricos ou pessoais. \n",
    "     \n",
    "\n",
    "III. Tendência de Postagens sobre Guerra  \n",
    "\n",
    "A análise temporal das postagens sobre \"Guerra\" revelou insights importantes: \n",
    "\n",
    "    Eventos externos:  Observamos picos de postagens diretamente relacionados a eventos significativos na guerra da Ucrânia, como ataques militares ou declarações de líderes políticos. Isso indica que a plataforma é sensível a notícias globais e serve como um espaço para discussão em tempo real. \n",
    "\n",
    "    Tendência geral:  Apesar dos picos, o volume médio de postagens sobre \"Guerra\" tem aumentado gradualmente ao longo do tempo, sugerindo que o interesse por esse tema permanece elevado. \n",
    "     \n",
    "\n",
    "IV. Conclusão  \n",
    "\n",
    "A análise dos dados permitiu identificar padrões claros de comportamento dos usuários e os principais temas discutidos na plataforma. Observamos que: \n",
    "\n",
    "    O volume de postagens é influenciado por eventos externos, como notícias globais e lançamentos de produtos culturais. \n",
    "\n",
    "    A categorização do conteúdo é uma ferramenta poderosa para entender tendências e interesses dos usuários, permitindo análises mais granulares e acionáveis. \n",
    "     \n",
    "\n",
    "Os resultados obtidos têm implicações práticas para diversas áreas, como monitoramento de mídias sociais, desenvolvimento de estratégias de engajamento e estudos sobre comportamento humano. Futuras análises poderiam explorar técnicas de processamento de linguagem natural (NLP) mais avançadas, como embeddings de palavras ou modelos de aprendizado profundo, para melhorar a precisão da classificação e capturar nuances adicionais nos dados. \n",
    "Sugestões para Aprimoramento  \n",
    "\n",
    "    Incorporação de NLP Avançado:  Utilize modelos de linguagem pré-treinados (ex.: BERT) para melhorar a classificação de temas e capturar contextos mais complexos. \n",
    "\n",
    "    Análise de Sentimentos:  Adicione uma camada de análise de sentimentos para entender o tom das discussões em cada categoria. \n",
    "\n",
    "    Visualizações Interativas:  Use bibliotecas como Plotly ou Dash para criar visualizações interativas que permitam explorar os dados de forma dinâmica. \n",
    "\n",
    "    Estudo de Caso:  Realize um estudo de caso focado em um evento específico (ex.: lançamento de um jogo ou escalada de um conflito) para entender como os usuários reagem em tempo real. \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Autoavaliação  \n",
    "\n",
    "O objetivo deste trabalho foi analisar e classificar postagens do Bluesky em diferentes temas, utilizando técnicas de processamento de texto e modelagem de dados. A seguir, apresento uma avaliação crítica dos principais aspectos do projeto, considerando os objetivos propostos, as decisões metodológicas e os resultados alcançados. \n",
    "\n",
    "1. Qualidade dos Dados  \n",
    "\n",
    "A qualidade dos dados foi um ponto central para garantir a confiabilidade das análises. As etapas de pré-processamento foram bem-sucedidas em lidar com inconsistências e ruídos nos dados: \n",
    "\n",
    "    Valores Ausentes:  A remoção de registros com valores ausentes na coluna text (menos de 1%) minimizou o impacto desses dados incompletos.\n",
    "    Detecção de Idioma:  O uso do modelo FastText para identificar idiomas foi eficaz, mas algumas postagens classificadas como \"unknown\" foram excluídas. Isso garantiu que apenas postagens em inglês fossem analisadas, alinhando-se ao escopo do projeto.\n",
    "    Limpeza de Texto:  A remoção de URLs, menções, hashtags e caracteres especiais foi crucial para eliminar ruído e focar no conteúdo relevante.\n",
    "    Padronização Temporal:  A conversão das datas para o fuso horário de São Paulo permitiu uma análise temporal consistente, especialmente para entender o comportamento dos usuários locais.\n",
    "     \n",
    "\n",
    "Embora essas etapas tenham melhorado significativamente a qualidade dos dados, é importante destacar que modelos de detecção de idioma podem falhar em textos curtos ou ambíguos. Uma possível melhoria seria combinar FastText com outras técnicas de detecção de idioma para aumentar a precisão. \n",
    "\n",
    "2. Solução do Problema  \n",
    "\n",
    "As perguntas-chave definidas no início do projeto foram respondidas de forma satisfatória: \n",
    "\n",
    "    Distribuição Temporal:  A análise revelou padrões claros de atividade, como picos à tarde e à noite, e maior engajamento às segundas e quintas-feiras. Esses insights podem ser úteis para estratégias de marketing e engajamento.\n",
    "    Principais Temas:  A categorização automática identificou temas predominantes, como Política, Videogames e Guerra. Essa classificação fornece uma visão valiosa sobre os interesses dos usuários e tendências globais.\n",
    "    Tendência de Postagens sobre Guerra:  A análise mostrou que eventos externos, como conflitos internacionais, influenciam diretamente o volume de discussões sobre guerra. Isso demonstra a sensibilidade da plataforma a notícias globais.\n",
    "     \n",
    "\n",
    "Apesar desses resultados positivos, a classificação atual ainda depende de palavras-chave específicas, o que pode limitar a captura de nuances contextuais. Incorporar técnicas mais avançadas de NLP, como embeddings de palavras ou modelos pré-treinados (ex.: BERT), poderia melhorar a precisão e capturar contextos mais complexos. \n",
    "\n",
    "3. Conclusão  \n",
    "\n",
    "Os resultados obtidos atenderam aos objetivos propostos e forneceram insights valiosos sobre o comportamento dos usuários e os principais temas discutidos no Bluesky. Observamos que: \n",
    "\n",
    "    O volume de postagens é altamente influenciado por eventos externos, como notícias globais e lançamentos culturais.\n",
    "    A categorização do conteúdo é uma ferramenta poderosa para entender tendências e interesses dos usuários, permitindo análises mais granulares e acionáveis.\n",
    "     \n",
    "\n",
    "Esses resultados têm implicações práticas para diversas áreas, como monitoramento de mídias sociais, desenvolvimento de estratégias de engajamento e estudos sobre comportamento humano. \n",
    "\n",
    "4. Limitações e Sugestões para Aprimoramento  \n",
    "\n",
    "Embora o projeto tenha sido bem-sucedido, algumas limitações devem ser consideradas: \n",
    "\n",
    "    Classificação de Temas:  A abordagem baseada em palavras-chave é limitada e pode não capturar todo o contexto de uma postagem. Incorporar modelos de linguagem pré-treinados, como BERT, melhoraria a precisão da classificação.\n",
    "    Análise de Sentimentos:  Adicionar uma camada de análise de sentimentos permitiria entender o tom das discussões em cada categoria, proporcionando insights mais profundos.\n",
    "    Visualizações Interativas:  Criar dashboards interativos com bibliotecas como Plotly ou Dash facilitaria a exploração dinâmica dos dados e tornaria os resultados mais acessíveis.\n",
    "    Estudo de Caso:  Realizar um estudo de caso focado em um evento específico (ex.: lançamento de um jogo ou escalada de um conflito) poderia fornecer uma compreensão mais detalhada de como os usuários reagem em tempo real.\n",
    "     \n",
    "\n",
    "5. Considerações Finais  \n",
    "\n",
    "Este projeto demonstrou que é possível extrair insights valiosos das postagens do Bluesky utilizando técnicas de processamento de texto e modelagem de dados. As análises realizadas forneceram uma visão clara dos padrões de comportamento dos usuários e dos principais temas discutidos na plataforma. \n",
    "\n",
    "No entanto, há espaço para melhorias, especialmente na incorporação de técnicas mais avançadas de NLP e na criação de visualizações interativas. Futuras análises poderiam explorar essas oportunidades para aprimorar ainda mais os resultados e expandir o escopo do projeto. \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 570111701458489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MVP_Engenharia_de_Dados",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
